model:
  path: outputs/merged_oracle_llama70b_awq4

server:
  host: 0.0.0.0
  port: 8000

generation:
  max_model_len: 8192
  max_num_seqs: 4
  max_tokens: 512
  temperature: 0.0
  top_p: 1.0

gpu:
  gpu_memory_utilization: 0.92

notes: |
  vLLM serving defaults for 1Ã—A100-80GB, 70B 4-bit quantized, 4 concurrent users.
  Tune based on actual latency and memory headroom.
  
  For production:
  - Measure actual memory footprint with `nvidia-smi`
  - Adjust gpu_memory_utilization if necessary (safe range: 0.85-0.95)
  - Consider reducing max_model_len or max_num_seqs if OOM occurs
  - Enable tensor parallel only if deploying on multiple GPUs
  
  Temperature=0.0 ensures deterministic SQL output (recommended).
